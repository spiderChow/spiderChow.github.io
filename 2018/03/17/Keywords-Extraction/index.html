<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  <title>Keywords Extraction | Home of Junying</title>
  <meta name="description" content="I would like to put my blogs and projects here, as well as something of worth." />
  <meta name="keywords" content="" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <link rel="shortcut icon" href="/images/favicon.ico">
  <link rel="alternate" href="/atom.xml" title="Home of Junying">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Finding some reliable algorithm in keywords extraction on a document of 300-500 words.">
<meta name="keywords" content="Keywords Extraction">
<meta property="og:type" content="article">
<meta property="og:title" content="Keywords Extraction">
<meta property="og:url" content="http://junying.ink/2018/03/17/Keywords-Extraction/index.html">
<meta property="og:site_name" content="Home of Junying">
<meta property="og:description" content="Finding some reliable algorithm in keywords extraction on a document of 300-500 words.">
<meta property="og:updated_time" content="2018-03-20T09:46:06.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Keywords Extraction">
<meta name="twitter:description" content="Finding some reliable algorithm in keywords extraction on a document of 300-500 words.">
    
  <link href="https://fonts.googleapis.com/css?family=Inconsolata|Titillium+Web" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Roboto+Mono" rel="stylesheet">
  <link href='//cdn.bootcss.com/node-waves/0.7.5/waves.min.css' rel='stylesheet'>


  <link rel="stylesheet" href="/style.css">
  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>
</head>

<body>
  <div id="loading-bar-wrapper">
  <div id="loading-bar"></div>
</div>


  <script>setLoadingBarProgress(20)</script> 
  <header class="l_header">
	<div class='wrapper'>
		<div class="nav-main container container--flex">
			<a class="logo flat-box" href='/' >
				Home of Junying
			</a>
			<div class='menu'>
				<ul class='h-list'>
					
						<li>
							<a class='flat-box nav-home' href='/'>
								Home
							</a>
						</li>
					
						<li>
							<a class='flat-box nav-archives' href='/archives'>
								Archives
							</a>
						</li>
					
						<li>
							<a class='flat-box nav-about' href='/about'>
								About
							</a>
						</li>
					
				</ul>
				<div class='underline'></div>
			</div>
			
				<div class="m_search">
					<form name="searchform" class="form u-search-form">
						<input type="text" class="input u-search-input" placeholder="Search" />
						<span class="icon icon-search"></span>
					</form>
				</div>
			
			<ul class='switcher h-list'>
				
					<li class='s-search'><a href='javascript:void(0)'><span class="icon icon-search flat-box"></span></a></li>
				
				<li class='s-menu'><a href='javascript:void(0)'><span class="icon icon-menu flat-box"></span></a></li>
			</ul>
		</div>
		
		<div class='nav-sub container container--flex'>
			<a class="logo" class="flat-box" href='javascript:void(0)'>
				Word of Forks
			</a>

			<ul class='switcher h-list'>
				<li class='s-comment'><a href='javascript:void(0)'><span class="icon icon-chat_bubble_outline flat-box"></span></a></li>
				<li class='s-top'><a href='javascript:void(0)'><span class="icon icon-arrow_upward flat-box"></span></a></li>
				<li class='s-toc'><a href='javascript:void(0)'><span class="icon icon-format_list_numbered flat-box"></span></a></li>
			</ul>
		</div>
	</div>
</header>
<aside class="menu-phone">
	<nav>
		
			<a href="/" class="nav-home nav">
				Home
			</a>
		
			<a href="/archives" class="nav-archives nav">
				Archives
			</a>
		
			<a href="/about" class="nav-about nav">
				About
			</a>
		
	</nav>
</aside>

    <script>setLoadingBarProgress(40);</script>
  <div class="l_body">
    <div class='container clearfix'>
      <div class='l_main'>
        <article id="post-Keywords-Extraction"
  class="post white-box article-type-post"
  itemscope itemprop="blogPost">
	<section class='meta'>
	<h2 class="title">
  	<a href="/2018/03/17/Keywords-Extraction/">
    	Keywords Extraction
    </a>
  </h2>
	<time>
	  Mar 17, 2018
	</time>
	
    
    <div class='cats'>
        <a href="/categories/NLP/">NLP</a>
    </div>

	</section>
	
		<section class="toc-wrapper"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Keyphrase-Extraction-Approaches"><span class="toc-number">1.</span> <span class="toc-text">Keyphrase Extraction Approaches</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Selecting-Candidate-Words-and-Phrases"><span class="toc-number">1.1.</span> <span class="toc-text">Selecting Candidate Words and Phrases</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Supervised-Approaches"><span class="toc-number">1.2.</span> <span class="toc-text">Supervised Approaches</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Task-Reformulation"><span class="toc-number">1.2.1.</span> <span class="toc-text">Task Reformulation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Feature-Design"><span class="toc-number">1.2.2.</span> <span class="toc-text">Feature Design</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Unsupervised-Approaches"><span class="toc-number">1.3.</span> <span class="toc-text">Unsupervised Approaches</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#0-The-ranking-based-on-tf-idf-has-been-shown-to-work-well-in-practice-Hasan-and-Ng-2014-2010-despite-its-simplicity"><span class="toc-number">1.3.1.</span> <span class="toc-text">0.The ranking based on tf-idf has been shown to work well in practice Hasan and Ng, 2014, 2010, despite its simplicity.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Graph-Based-Ranking"><span class="toc-number">1.3.2.</span> <span class="toc-text">1. Graph-Based Ranking</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Topic-Based-Clustering"><span class="toc-number">1.3.3.</span> <span class="toc-text">2. Topic-Based Clustering</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Simultaneous-Learning"><span class="toc-number">1.3.4.</span> <span class="toc-text">3. Simultaneous Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Language-Modeling"><span class="toc-number">1.3.5.</span> <span class="toc-text">4. Language Modeling</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Evaluation"><span class="toc-number">2.</span> <span class="toc-text">Evaluation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Dataset"><span class="toc-number">2.1.</span> <span class="toc-text">Dataset:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Metrics"><span class="toc-number">2.2.</span> <span class="toc-text">Metrics:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-state-of-Art"><span class="toc-number">2.3.</span> <span class="toc-text">The state of Art</span></a></li></ol></li></ol></section>
	
	<section class="article typo">
  	<div class="article-entry" itemprop="articleBody">
    	<p>Finding some reliable algorithm in keywords extraction on a document of 300-500 words.<br><a id="more"></a></p>
<p><a href="http://acl2014.org/acl2014/P14-1/pdf/P14-1119.pdf" target="_blank" rel="noopener">Automatic Keyphrase Extraction: A Survey of the State of the Art</a> by Kazi Saidul Hasan and Vincent Ng, 2014<br><a href="http://aclweb.org/anthology/P17-1102" target="_blank" rel="noopener">PositionRank: An Unsupervised Approach to Keyphrase Extraction from Scholarly Documents</a><br><a href="http://aclweb.org/anthology/D/D16/D16-1191.pdf" target="_blank" rel="noopener">A Graph Degeneracy-based Approach to Keyword Extraction</a></p>
<h1 id="Keyphrase-Extraction-Approaches"><a href="#Keyphrase-Extraction-Approaches" class="headerlink" title="Keyphrase Extraction Approaches"></a>Keyphrase Extraction Approaches</h1><p>Two steps: </p>
<ul>
<li>extracting a list of words/phrases that serve as candidate keyphrases using some heuristics  </li>
<li>determining which of these candidate keyphrases are correct keyphrases using <code>supervised</code>  or <code>unsupervised</code> approaches.</li>
</ul>
<h2 id="Selecting-Candidate-Words-and-Phrases"><a href="#Selecting-Candidate-Words-and-Phrases" class="headerlink" title="Selecting Candidate Words and Phrases"></a>Selecting Candidate Words and Phrases</h2><p>These rules are designed to avoid spurious instances and keep the number of candidates to a minimum.<br>Typical heuristics include:  </p>
<ol>
<li>using a <code>stop word</code> list to remove stop words <a href="">Liu et al., 2009b</a>,   </li>
<li>allowing words with certain <code>part-of-speech tags</code> (e.g., nouns, adjectives, verbs) to be candidate keywords <a href="">Mihalcea and Tarau, 2004; Wan and Xiao, 2008b; Liu et al., 2009a</a>,   </li>
<li>allowing n-grams that appear in <code>Wikipedia article titles</code> to be candidates <a href="">Grineva et al., 2009</a>,  </li>
<li>extracting n-grams <a href="">Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009</a> or noun phrases <a href="">Barker and Cornacchia, 2000; Wu et al., 2005</a> that satisfy <code>pre-defined lexico-syntactic pattern(s)</code> <a href="">Nguyen and Phan, 2009</a>.</li>
</ol>
<p>Different pruning heuristics have been designed to <code>prune</code> candidates that are unlikely to be keyphrases (Huang et al., 2006; Kumar and Srinathan, 2008; El-Beltagy and Rafea, 2009; You et al., 2009; Newman et al., 2012).</p>
<hr>
<h2 id="Supervised-Approaches"><a href="#Supervised-Approaches" class="headerlink" title="Supervised Approaches"></a>Supervised Approaches</h2><p>keyphrase extraction is formulated as a binary classification problem, where candidate phrases are classified as either positive (i.e., keyphrases) or negative (i.e., non-keyphrases).  </p>
<p>Research on supervised approaches to keyphrase extraction has focused on two issues: <code>task reformulation</code> and <code>feature design</code>. </p>
<h3 id="Task-Reformulation"><a href="#Task-Reformulation" class="headerlink" title="Task Reformulation"></a>Task Reformulation</h3><h3 id="Feature-Design"><a href="#Feature-Design" class="headerlink" title="Feature Design"></a>Feature Design</h3><hr>
<h2 id="Unsupervised-Approaches"><a href="#Unsupervised-Approaches" class="headerlink" title="Unsupervised Approaches"></a>Unsupervised Approaches</h2><p>Four Group:</p>
<h3 id="0-The-ranking-based-on-tf-idf-has-been-shown-to-work-well-in-practice-Hasan-and-Ng-2014-2010-despite-its-simplicity"><a href="#0-The-ranking-based-on-tf-idf-has-been-shown-to-work-well-in-practice-Hasan-and-Ng-2014-2010-despite-its-simplicity" class="headerlink" title="0.The ranking based on tf-idf has been shown to work well in practice Hasan and Ng, 2014, 2010, despite its simplicity."></a>0.The ranking based on <code>tf-idf</code> has been shown to work well in practice <a href="">Hasan and Ng, 2014, 2010</a>, despite its simplicity.</h3><h3 id="1-Graph-Based-Ranking"><a href="#1-Graph-Based-Ranking" class="headerlink" title="1. Graph-Based Ranking"></a>1. Graph-Based Ranking</h3><p>Importance :a candidate is important if it is related to (1) a large number of candidates and (2) candidates that are important.  </p>
<ol>
<li>build a graph: node -&gt; candidate keyphrase; edge -&gt; relateness; weight -&gt; Syntactic or/and Semantic relatedness</li>
<li>rank the node via graph-based ranking methods: egde -&gt; vote;   <blockquote>
<p>A node’s score in the graph is defined recursively in terms of the edges it has and the scores of the neighboring nodes.</p>
</blockquote>
</li>
</ol>
<ul>
<li><code>TextRank</code> <a href="">Mihalcea and Tarau, 2004</a><blockquote>
<p>Applying PageRank on a word graph built from adjacent words within a document.<br>the nodes of graphs-of-words are ranked based on a modified version of the PageRank algorithm taking edge weights into account, and the top p% vertices are kept as keywords.<br>It does not guarantee that all the main topics will be represented by the extracted keyphrases.</p>
</blockquote>
</li>
<li><code>SingleRank</code><a href="">Wan and Xiao 2008</a><blockquote>
<p>Extended TextRank by adding weighted edges between words that co-occur in a window of variable size w ≥ 2.</p>
</blockquote>
</li>
<li><code>ExpandRank</code><a href="">Wan and Xiao 2008</a><blockquote>
<p>Textually-similar neighboring documents are included in ExpandRank (Wan and Xiao, 2008) to compute more accurate word co-occurrence information</p>
</blockquote>
</li>
</ul>
<p>Researchers have computed relatedness between candidates using <code>co-occurrence counts</code> (Mihalcea and Tarau, 2004; Matsuo and Ishizuka, 2004) and <code>semantic relatedness</code> (Grineva et al., 2009), and represented the relatedness information collected from a document as a graph (Mihalcea and Tarau, 2004; Wan and Xiao, 2008a; Wan and Xiao, 2008b; Bougouin et al., 2013).</p>
<h3 id="2-Topic-Based-Clustering"><a href="#2-Topic-Based-Clustering" class="headerlink" title="2. Topic-Based Clustering"></a>2. Topic-Based Clustering</h3><ul>
<li><p><code>KeyCluster</code>: clusters semantically similar candidates using Wikipedia and co-occurrence-based statis- tics. <a href="">Clustering to find exemplar terms for keyphrase extraction. Liu et al. (2009b)</a></p>
<blockquote>
<p>first grouping candidate words into topics and then, extracting one representative keyphrase from each topic.<br>drawback: by extracting keyphrases from each topic cluster, it essentially gives each topic equal importance. </p>
</blockquote>
</li>
<li><p><code>Topical PageRank</code> <code>(TPR)</code>: <a href="http://www.personal.psu.edu/wzh112/publications/emnlp2010.pdf" target="_blank" rel="noopener">Automatic keyphrase extraction via topic decomposition. Liu 2010</a> overcome drawbacks of KeyCluster.</p>
<blockquote>
<p>In particular, they decomposed a document into multiple topics, using topic models, and applied a separate topic-biased PageRank for each topic. The PageRank scores from each topic were then combined into a single score, using as weights the topic proportions returned by topic models for the document.<br>TPR performs significantly better than both <code>tf*idf</code> and <code>TextRank</code> on the DUC-2001 and Inspec datasets.</p>
</blockquote>
</li>
<li><p><code>CommunityCluster</code>: <a href="http://www2009.eprints.org/67/1/p661.pdf" target="_blank" rel="noopener">Extracting Key Terms From Noisy and Multi-theme Documents,Grineva et al. (2009)</a></p>
<blockquote>
<p>CommunityCluster gives more weight to more important topics, but unlike TPR, it extracts all candidate keyphrases from an important topic, assuming that a candidate that receives little focus in the text should still be extracted as a keyphrase as long as it is related to an important topic. </p>
</blockquote>
</li>
</ul>
<h3 id="3-Simultaneous-Learning"><a href="#3-Simultaneous-Learning" class="headerlink" title="3. Simultaneous Learning"></a>3. Simultaneous Learning</h3><h3 id="4-Language-Modeling"><a href="#4-Language-Modeling" class="headerlink" title="4. Language Modeling"></a>4. Language Modeling</h3><p><em>deserves further investigation</em><br><a href="">Tomokiyo and Hurst 2003</a><br>Language Model<br>The foreground corpus is composed of the set of documents from which keyphrases are to be extracted.<br>The background corpus is a large corpus that encodes general knowledge about the world (e.g., the Web).</p>
<p>LMA uses a language model rather than heuristics to identify phrases, and relies on the language model trained on the background corpus to determine how “unique” a candidate keyphrase is to the domain represented by the foreground corpus. </p>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset:"></a>Dataset:</h2><ul>
<li><p>Inspec (Hulth, 2003): This dataset provides 2,000 paper abstracts. We adopt the 500 testing papers and their corresponding uncontrolled keyphrases for evaluation, and the remaining 1,500 papers are used for training the supervised baseline models. </p>
<blockquote>
<p>The Hulth 2003 <a href="https://github.com/snkim/ AutomaticKeyphraseExtraction" target="_blank" rel="noopener">Hulth, 2003</a> dataset contains abstracts drawn from the Inspec database of physics and engineering papers. Following our baselines, we used the 500 documents in the validation set and the “uncontrolled” keywords assigned by human annotators. The mean document size is 120 words and on average, 21 keywords (in terms of unigrams) are available for each document.</p>
</blockquote>
</li>
<li><p><a href="https://github.com/snkim/ AutomaticKeyphraseExtraction" target="_blank" rel="noopener">Marujo 2012</a>, containing 450 web news stories of about <code>440 words on average</code>, covering 10 different topics from art and culture to business, sport, and technology (Marujo et al., 2012). </p>
<blockquote>
<p>For each story, the keyphrases assigned by at least 9 out of 10 Amazon Mechanical Turkers are provided as gold standard. After splitting the keyphrases into unigrams, this makes for an average of 68 keywords per document, which is much higher than for the two other datasets, even the one comprising long documents (Semeval, see next). </p>
</blockquote>
</li>
</ul>
<ul>
<li><p>Krapivin (Krapivin et al., 2008): This dataset provides 2,304 papers with full-text and author-assigned keyphrases. However, the author did not mention how to split testing data, so we selected the first 400 papers in alphabetical order as the testing data, and the remaining papers are used to train the su- pervised baselines.</p>
</li>
<li><p>NUS (Nguyen and Kan, 2007): We use the author-assigned keyphrases and treat all 211 papers as the testing data. Since the NUS dataset did not specifically mention the ways of splitting training and testing data, the results of the supervised baseline models are obtained through a five-fold cross-validation. </p>
</li>
<li><p>SemEval-2010 (Kim et al., 2010)<a href="https://github.com/boudinfl/centrality_ measures_ijcnlp13/tree/master/data" target="_blank" rel="noopener">Semeval</a>: parsed scientific papers collected from the ACM Digital Library. Each document is approximately 1,860 words in length and is associated with about 24 keywords.</p>
</li>
</ul>
<ul>
<li>KP20k: We built a new testing dataset that contains the titles, abstracts, and keyphrases of 20,000 scientific articles in computer science. They were randomly selected from our obtained 567,830 articles. Due to the memory limits of implementation, we were not able to train the supervised baselines on the whole training set. Thus we take the 20,000 articles in the validation set to train the supervised baselines. It is worth noting that we also examined their performance by enlarging the training dataset to 50,000 articles, but no significant improvement was observed.</li>
</ul>
<h2 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics:"></a>Metrics:</h2><p>Typical approach:<br>(1) to create a mapping between the keyphrases in the gold standard and those in the system output using exact match, and then (2) score the output using evaluation metrics such as <code>precision (P), recall (R), and F-score</code></p>
<p>Two types of automatic evaluation metrics:  </p>
<ul>
<li><p>With <code>exact match</code>. </p>
<blockquote>
<p>   These metrics reward a <code>partial match</code> between a predicted keyphrase and a <code>gold</code> keyphrase (i.e., overlapping n-grams) and are commonly used in <code>machine translation (MT)</code> and <code>summarization evaluations</code>. They include <strong>BLEU</strong>, <strong>METEOR</strong>, <strong>NIST</strong>, and <strong>ROUGE</strong>. </p>
<blockquote>
<p>   Nevertheless, experiments show that these MT metrics only offer a partial solution to problem with exact match: they can only detect a subset of the near-misses (Kim et al., 2010a).</p>
</blockquote>
</blockquote>
</li>
<li><p><code>How a system ranks</code> its predictions: </p>
</li>
</ul>
<blockquote>
<ol>
<li>Given that two systems A and B have the same number of correct predictions, binary preference measure (Bpref) and mean reciprocal rank (MRR) <a href="">Liu et al., 2010</a> will award more credit to A than to B <strong><em>if the ranks of the correct predictions in A’s output are higher than those in B’s output</em></strong>.</li>
</ol>
</blockquote>
<blockquote>
<ol>
<li>R-precision (Rp): IR metric that focuses on ranking: given a document with n gold keyphrases, it computes the precision of a system over its n highest-ranked candidates <a href="">Zesch and Gurevych, 2009</a>. <blockquote>
<p>The motivation behind the design of Rp is simple: a system will achieve a perfect Rp value if it ranks all the keyphrases above the non-keyphrases.</p>
</blockquote>
</li>
</ol>
</blockquote>
<h2 id="The-state-of-Art"><a href="#The-state-of-Art" class="headerlink" title="The state of Art"></a>The state of Art</h2>
  	</div>
	  
	  <div class="article-tags tags">
      
        <a href="/tags/Keywords-Extraction/">Keywords Extraction</a>
      
	  </div>
    
		
	
		<div class="art-item-footer">
				
					<span class="art-item-left"><i class="icon icon-chevron-thin-left"></i>prev：<a href="/2018/03/19/keys-extraction-algorithm/" rel="prev"  title="keys extraction algorithm">
						keys extraction algorithm 
					</a></span>
				
				
					<span class="art-item-right">next：<a href="/2018/03/16/Linux-Terminal-Notes/" rel="next"  title="Linux Terminal Notes">
						Linux Terminal Notes
					</a><i class="icon icon-chevron-thin-right"></i></span>
				
		</div>
	
	</section>
	
</article>
<script>
	window.subData = {
		title: 'Keywords Extraction',
		tools: true
	}
</script>

      </div>
      <aside class='l_side'>
        
  <section class='m_widget about'>

<img class='avatar waves-image' src='/images/avatar.jpg' />

<div class='header'>Junying Chou</div>
<div class='content'>
<div class='desc'>Practise swimming and coding!</div>
</div>
</section>

  <section class='m_widget categories'>
<div class='header'>Categories</div>
<div class='content'>
    
    <ul class="entry">
    
        <li><a class="flat-box" href="/categories/CS-61B/"><div class='name'>CS 61B</div><div class='badget'>1</div></a></li>
    
        <li><a class="flat-box" href="/categories/Hexo/"><div class='name'>Hexo</div><div class='badget'>2</div></a></li>
    
        <li><a class="flat-box" href="/categories/Markdown/"><div class='name'>Markdown</div><div class='badget'>1</div></a></li>
    
        <li><a class="flat-box" href="/categories/NLP/"><div class='name'>NLP</div><div class='badget'>3</div></a></li>
    
    </ul>
    
</div>
</section>

  
<div class="m_widget tagcloud">
    <div class="header">Tags</div>
    <div class='content'>
        <a href="/tags/Hexo/" style="font-size: 14px; color: #808080">Hexo</a> <a href="/tags/Java/" style="font-size: 20px; color: #000">Java</a> <a href="/tags/Keywords-Extraction/" style="font-size: 20px; color: #000">Keywords Extraction</a> <a href="/tags/Linux/" style="font-size: 14px; color: #808080">Linux</a> <a href="/tags/Machine-learning/" style="font-size: 14px; color: #808080">Machine learning</a> <a href="/tags/Markdown/" style="font-size: 14px; color: #808080">Markdown</a>
    </div>
</div>



      </aside>
      <script>setLoadingBarProgress(60);</script>
    </div>
  </div>
  <footer id="footer" class="clearfix">

	<div class="social-wrapper">
  	
      
        <a href="https://github.com/spiderChow" class="social github"
          target="_blank" rel="external">
          <span class="icon icon-github"></span>
        </a>
      
    
  </div>
  
  <div>Theme <a href='https://github.com/stkevintan/hexo-theme-material-flow' class="codename">MaterialFlow</a> designed by <a href="http://keyin.me/" target="_blank">Kevin Tan</a>.</div>
  
</footer>


  <script>setLoadingBarProgress(80);</script>
  

<script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>
<script src='//cdn.bootcss.com/node-waves/0.7.5/waves.min.js'></script>
<script src="//cdn.bootcss.com/scrollReveal.js/3.3.2/scrollreveal.min.js"></script>
<script src="/js/jquery.fitvids.js"></script>
<script>
	var GOOGLE_CUSTOM_SEARCH_API_KEY = "";
	var GOOGLE_CUSTOM_SEARCH_ENGINE_ID = "";
	var ALGOLIA_API_KEY = "";
	var ALGOLIA_APP_ID = "";
	var ALGOLIA_INDEX_NAME = "";
  var AZURE_SERVICE_NAME = "";
  var AZURE_INDEX_NAME = "";
  var AZURE_QUERY_KEY = "";
  var BAIDU_API_ID = "";
  var SEARCH_SERVICE = "hexo";
  var ROOT = "/"||"/";
  if(!ROOT.endsWith('/'))ROOT += '/';
</script>
<script src="/js/search.js"></script>
<script src="/js/app.js"></script>


  <script>setLoadingBarProgress(100);</script>
</body>
</html>
