{"meta":{"title":"Home of Junying","subtitle":null,"description":"I would like to put my blogs and projects here, as well as something of worth.","author":"Junying Chou","url":"http://junying.ink"},"pages":[{"title":"about","date":"2018-03-16T08:37:36.000Z","updated":"2018-03-16T14:53:58.000Z","comments":true,"path":"about/index.html","permalink":"http://junying.ink/about/index.html","excerpt":"","text":"希望世界和平My name is Junying Chou.Email: qunicychow@gmail.com I am studying the CS 61B and take notes for this class.I also try to finish some leetcode problems and write down some notes."},{"title":"tags","date":"2018-03-15T14:07:50.000Z","updated":"2018-03-16T08:28:31.000Z","comments":false,"path":"tags/index.html","permalink":"http://junying.ink/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2018-03-16T08:30:37.000Z","updated":"2018-03-16T08:30:52.000Z","comments":true,"path":"categories/index.html","permalink":"http://junying.ink/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"vim","slug":"vim","date":"2018-04-02T00:51:03.000Z","updated":"2018-04-02T00:53:32.000Z","comments":true,"path":"2018/04/02/vim/","link":"","permalink":"http://junying.ink/2018/04/02/vim/","excerpt":"","text":":set number #设置行号:%s/old/new/g #搜索整个文件，将所有的old替换为new:%s/old/new/gc 搜索整个文件，将所有的old替换为new，每次都要你确认是否替换 h,j,k,l 上，下，左，右ctrl-f 上翻一页ctrl-b 下翻一页$ 跳至行尾","categories":[{"name":"vim","slug":"vim","permalink":"http://junying.ink/categories/vim/"}],"tags":[]},{"title":"augular5","slug":"augular5","date":"2018-04-01T13:43:06.000Z","updated":"2018-04-01T14:17:57.000Z","comments":true,"path":"2018/04/01/augular5/","link":"","permalink":"http://junying.ink/2018/04/01/augular5/","excerpt":"","text":"set up your project1234npm install -g @angular/cling new my-first-appcd my-frist-appng server create component mkdir under the app folder called ‘server’ under server/ create&ensp;server.component.ts&ensp;server.component.css&ensp;server.component.html in the server.component.ts 12345678import &#123; Component &#125; from '@angular/core';@Component(&#123; selector: 'app-server', templateUrl: './server.component.html', &#125;)export class ServerComponent&#123; &#125; in the app.module.tsadd ‘ServerComponent’ in the declarationand import {ServerComponent} from ‘./server/server.component’; or use CLI:ng g c server use the componentin app.component.html just add","categories":[],"tags":[{"name":"udemy","slug":"udemy","permalink":"http://junying.ink/tags/udemy/"}]},{"title":"Lists","slug":"Lists","date":"2018-03-27T15:14:30.000Z","updated":"2018-03-27T23:56:17.000Z","comments":true,"path":"2018/03/27/Lists/","link":"","permalink":"http://junying.ink/2018/03/27/Lists/","excerpt":"Something about the types and delcaration and InstantiationAnd implement the List Class and its get(), size()","text":"Something about the types and delcaration and InstantiationAnd implement the List Class and its get(), size() Something about the types and delcaration and InstantiationmemoryYour computer stores information in “memory”.In Java, there are 8 primitive types: byte, short, int, long, float, double, boolean, and char. One interesting observation is that both 72 and H are stored as 01001000.This raises the question: how does a piece of Java code know how to interpret 01001000?The answer is through types! Java interpreter treats them differently when printed. delcareWhen you declare a variable of a certain type, Java finds a contiguous block of exactly enough bits to hold a thing of that type. For example, if you declare an int, you get a block of 32 bits. Java creates an internal table that maps each variable name to a location. Java does not write anything into the reserved box when a variable is declared. In other words, there are no default values. As a result, the Java compiler prevents you from using a variable until after the box has been filled with bits using the = operator. The Golden Rule of Equals (GRoE)When you write y = x, you are telling the Java interpreter to copy the bits from x into y. referenceEverything else, including arrays, is not a primitive type but rather a reference type Class InstantiationsWhen we instantiate an Object (e.g. Dog, Walrus, Planet): Java first allocates a box of bits for each instance variable of the class and fills them with a default value (e.g. 0, null). The constructor then usually fills every such box with some other value.12Walrus someWalrus;someWalrus = new Walrus(1000, 8.3); Reference Type Variable DeclarationsWhen we declare a variable of any reference type (Walrus, Dog, Planet):Java allocates exactly a box of size 64 bits, no matter what type of object.These bits can be either set to: Null (all zeros). The 64 bit “address” of a specific instance of that class (returned by new). Reference Types Obey the Golden Rule of Equalscopy the address Parameter PassingIn Java, we always pass by value. The golden rule:b = a copies the bits from a into b.Passing parameters copies the bits. Instantiation of ArraysArrays are also Objects. As we’ve seen, objects are (usually) instantiated using the new keyword.12Planet p = new Planet(0, 0, 0, 0, 0, “blah.png”);int[] x = new int[]&#123;0, 1, 2, 95, 4&#125;; int[] a = new int[]{0, 1, 2, 95, 4};Creates a 64 bit box for storing an int array address. (declaration)Creates a new Object, in this case an int array. (instantiation)Puts the address of this new Object into the 64 bit box named a. (assignment) Note: Instantiated objects can be lost! Build our own list class123456789public class IntList &#123; public int first; public IntList rest; public IntList(int f, IntList r) &#123; first = f; rest = r; &#125;&#125; For example, if we want to make a list of the numbers 5, 10, and 15, we can either do:123IntList L = new IntList(5, null);L.rest = new IntList(10, null);L.rest.rest = new IntList(15, null); Alternately, we could build our list backwards, yielding slightly nicer but harder to understand code: 123IntList L = new IntList(15, null);L = new IntList(10, L);L = new IntList(5, L); size()1234567891011121314151617/** Return the size of the list using... recursion! */public int size() &#123; if (rest == null) &#123; return 1; &#125; return 1 + this.rest.size();&#125;/** Return the size of the list using no recursion! */public int iterativeSize() &#123; IntList p = this; int totalSize = 0; while (p != null) &#123; totalSize += 1; p = p.rest; &#125; return totalSize;&#125; The key thing to remember about recursive code is that you need a base case. In this situation, the most reasonable base case is that rest is null, which results in a size 1 list. get()","categories":[{"name":"CS 61B","slug":"CS-61B","permalink":"http://junying.ink/categories/CS-61B/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://junying.ink/tags/Java/"},{"name":"List","slug":"List","permalink":"http://junying.ink/tags/List/"}]},{"title":"entropy","slug":"entropy","date":"2018-03-26T05:33:21.000Z","updated":"2018-03-26T06:03:25.000Z","comments":true,"path":"2018/03/26/entropy/","link":"","permalink":"http://junying.ink/2018/03/26/entropy/","excerpt":"entropy and cross entropy and relative entropy, pointwise mutual information","text":"entropy and cross entropy and relative entropy, pointwise mutual information 知乎上对于交叉熵信息熵相对熵的理解 交叉熵，其用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小。相对熵，其用来衡量两个取值为正的函数或概率分布之间的差异相对熵 = 某个策略的交叉熵 - 信息熵（根据系统真实分布计算而得的信息熵，为最优策略 PMI（Pointwise Mutual Information）$$PMI(x;y)=log p(x,y)/p(x)p(y)=log p(x|y)/p(x)=logp(y|x)/p(y)$$ 如果x跟y不相关，则p(x,y)=p(x)p(y)。二者相关性越大，则p(x,y)就相比于p(x)p(y)越大 例子举个自然语言处理中的例子来说，我们想衡量like这个词的极性（正向情感还是负向情感）。我们可以预先挑选一些正向情感的词，比如good。然后我们算like跟good的PMI，即： PMI(like,good)=logp(like,good)p(like)p(good)其中p(like)是like在语料库中出现的概率（出现次数除以总词数N），p(like,good)表示like跟good在一句话中同时出现的概率（like跟good同时出现的次数除以N2）。PMI(like,good)越大表示like的正向情感倾向就越明显","categories":[{"name":"Maths","slug":"Maths","permalink":"http://junying.ink/categories/Maths/"}],"tags":[]},{"title":"LDA","slug":"LDA","date":"2018-03-26T02:11:24.000Z","updated":"2018-03-26T05:56:10.000Z","comments":true,"path":"2018/03/26/LDA/","link":"","permalink":"http://junying.ink/2018/03/26/LDA/","excerpt":"gggg","text":"gggg","categories":[{"name":"NLP","slug":"NLP","permalink":"http://junying.ink/categories/NLP/"}],"tags":[]},{"title":"Stanford Ner","slug":"Stanford-Ner","date":"2018-03-20T05:32:23.000Z","updated":"2018-03-26T02:10:49.000Z","comments":true,"path":"2018/03/20/Stanford-Ner/","link":"","permalink":"http://junying.ink/2018/03/20/Stanford-Ner/","excerpt":"To find how to train the model incrementally, e.g. if the client add a sentence, then a updated model should be commited. For efficience, we should only train the new sentence instead of train all the corpus again.","text":"To find how to train the model incrementally, e.g. if the client add a sentence, then a updated model should be commited. For efficience, we should only train the new sentence instead of train all the corpus again. Likelihood:","categories":[{"name":"NLP","slug":"NLP","permalink":"http://junying.ink/categories/NLP/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://junying.ink/tags/Java/"},{"name":"Machine learning","slug":"Machine-learning","permalink":"http://junying.ink/tags/Machine-learning/"},{"name":"CRF","slug":"CRF","permalink":"http://junying.ink/tags/CRF/"}]},{"title":"keys extraction algorithm","slug":"keys-extraction-algorithm","date":"2018-03-19T02:47:46.000Z","updated":"2018-03-20T11:37:08.000Z","comments":true,"path":"2018/03/19/keys-extraction-algorithm/","link":"","permalink":"http://junying.ink/2018/03/19/keys-extraction-algorithm/","excerpt":"TextRank, TPR, RAKE","text":"TextRank, TPR, RAKE TextRankTextRank: Bringing Order into Texts2004 EMNLP, Rada Mihalcea and Paul Tarau $$S(V_i) = (1-d)+d*\\Sigma_{j\\in IN(V_i)} S(V_j)/|Out(V_j)|$$ Graph: Vertice: syntactic filters select only lexical units of a certain part of speech. One can for instance con- sider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verbs. Edge: We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words. Co-occurrence links express relations between syntactic element. Steps: The text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters. After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described is run on the graph for several iterations until it converges – usually for 20-30 iterations, at a threshold of 0.0001. Top vertices in the ranking are retained for post-processing. Sequences of adjacent keywords are collapsed into a multi-word keyword. Topical PageRankAutomatic keyphrase extraction via topic decomposition. Liu 20102010 EMNLP, Zhiyuan Liu, Wenyi Huang, Yabin Zheng and Maosong Sun Word graph:The link directions are determined as follows. When sliding a W-width window, at each position, we add links from the first word pointing to other words within the window. Since keyphrases are usually noun phrases, we only add adjectives and nouns in word graph. LDA:N 文章长度，满足泊松分布（越长越少，越短越少）一个文章的topic分布 θ ~ Dir(alpha) alpha是topic的一个分布情况。Dir: 将一个分布换成另一个分布 topic z_n ~multinomial(θ)w ~ p(w|z_n,beta)似然函数变分推断，EM算法(含有隐蔽那辆的时候求最大似然)VAE: 似然 x-&gt;y ，引入隐变量 x-&gt;h-&gt;y,利用琴生不等式，变成 含有KL距离 The candidate keyphrases of a document is obtained as follows: The document is first tokenized. After that, we annotate the document with (POS) tags. Third, we extract noun phrases with pattern (adjective)* (noun)+. the ranking score of a candidate keyphrase is computed by summing up the ranking scores of all words within the phrase. PageRank –&gt; Topical PageRank $$R(V_i) = (1-\\lambda)(1/|V|)+\\lambda \\Sigma_{j\\in IN(V_i)} R(V_j)* weight(V_j,V_i)/|Out(V_j)|$$ ==&gt; Biased PageRank: not equal probabilities of random jump to all vertices. $$R_z(V_i) = (1-\\lambda)p_z(V_i)+\\lambda \\Sigma_{j\\in IN(V_i)} R_Z(V_j)* weight(V_j,V_i)/|Out(V_j)|$$ Topic distribution of each word: pr(z|w)LDA: unsurpvised$p_z(V_i)$=pr(z|w) or pr(w|z) or pr(z|w)pr(w|z) In TPR for keyphrase extraction, we first compute the ranking scores of candidate keyphrases separately for each topic.For each candidate keyphrase , we compute its final ranking score as $$R(p)=\\Sigma_zR_z(p)*pr(z|d)$$ Rapid automatic keyword extraction (RAKE)Automatic keyword extraction from individual documents, Stuart Rose, Dave Engel, Nick Cramer and Wendy Cowley, 2010 very efficient~ The above paper even has a good example. Candidate keywords First, the document text is split into an array of words by the specified word delimiters.This array is then split into sequences of contiguous words at phrase delimiters and stop word positions. Word co-occurrences Graph After get the nodes of the graph, we would add the edges corresponding with the word co-occurrences.The graph is represented by a Matrix.Elements of the matrix is the word co-occurrences counts. Score of candidiate (sum of its member word scores): We evaluated several metrics for calculating word scores, based on the degree and frequency of word vertices in the graph:ratio of degree to frequency (deg(w)/freq(w)).(I think the ratio is similar with the TFIDF) Adjoining keywordsA new candidate keyword is then created as a combination of those keywords and their interior stop words. Top Tone of third EvaluationThe collection consists of 2000 Inspec abstracts for journal papers from Computer Science and Information Technology. The abstracts are divided into a training set with 1000 abstracts, a validation set with 500 abstracts, and a testing set with 500 abstracts. We followed the approach described in Mihalcea and Tarau (2004), using the testing set for evaluation because RAKE does not require a training set. Extracted keywords for each abstract are compared against the abstract’s associated set of manually assigned uncontrolled keywords. Single document keyphrase extraction using neighborhood knowledgeAAAI 2008Single document keyphrase extraction using neighborhood knowledge, Xiaojun Wan and Jianguo Xiao 2008 Abstract: This paper proposes to use a small number of nearest neighbor documents to provide more knowledge to improve single document keyphrase extraction. A specified document is expanded to a small document set by adding a few neighbor documents close to the document, and the graph-based ranking algorithm is then applied on the expanded document set to make use of both the local information in the specified document and the global information in the neighbor documents","categories":[{"name":"NLP","slug":"NLP","permalink":"http://junying.ink/categories/NLP/"}],"tags":[{"name":"Keywords Extraction","slug":"Keywords-Extraction","permalink":"http://junying.ink/tags/Keywords-Extraction/"}]},{"title":"Keywords Extraction","slug":"Keywords-Extraction","date":"2018-03-17T00:34:17.000Z","updated":"2018-03-20T09:46:06.000Z","comments":true,"path":"2018/03/17/Keywords-Extraction/","link":"","permalink":"http://junying.ink/2018/03/17/Keywords-Extraction/","excerpt":"Finding some reliable algorithm in keywords extraction on a document of 300-500 words.","text":"Finding some reliable algorithm in keywords extraction on a document of 300-500 words. Automatic Keyphrase Extraction: A Survey of the State of the Art by Kazi Saidul Hasan and Vincent Ng, 2014PositionRank: An Unsupervised Approach to Keyphrase Extraction from Scholarly DocumentsA Graph Degeneracy-based Approach to Keyword Extraction Keyphrase Extraction ApproachesTwo steps: extracting a list of words/phrases that serve as candidate keyphrases using some heuristics determining which of these candidate keyphrases are correct keyphrases using supervised or unsupervised approaches. Selecting Candidate Words and PhrasesThese rules are designed to avoid spurious instances and keep the number of candidates to a minimum.Typical heuristics include: using a stop word list to remove stop words Liu et al., 2009b, allowing words with certain part-of-speech tags (e.g., nouns, adjectives, verbs) to be candidate keywords Mihalcea and Tarau, 2004; Wan and Xiao, 2008b; Liu et al., 2009a, allowing n-grams that appear in Wikipedia article titles to be candidates Grineva et al., 2009, extracting n-grams Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009 or noun phrases Barker and Cornacchia, 2000; Wu et al., 2005 that satisfy pre-defined lexico-syntactic pattern(s) Nguyen and Phan, 2009. Different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases (Huang et al., 2006; Kumar and Srinathan, 2008; El-Beltagy and Rafea, 2009; You et al., 2009; Newman et al., 2012). Supervised Approacheskeyphrase extraction is formulated as a binary classification problem, where candidate phrases are classified as either positive (i.e., keyphrases) or negative (i.e., non-keyphrases). Research on supervised approaches to keyphrase extraction has focused on two issues: task reformulation and feature design. Task ReformulationFeature Design Unsupervised ApproachesFour Group: 0.The ranking based on tf-idf has been shown to work well in practice Hasan and Ng, 2014, 2010, despite its simplicity.1. Graph-Based RankingImportance :a candidate is important if it is related to (1) a large number of candidates and (2) candidates that are important. build a graph: node -&gt; candidate keyphrase; edge -&gt; relateness; weight -&gt; Syntactic or/and Semantic relatedness rank the node via graph-based ranking methods: egde -&gt; vote; A node’s score in the graph is defined recursively in terms of the edges it has and the scores of the neighboring nodes. TextRank Mihalcea and Tarau, 2004 Applying PageRank on a word graph built from adjacent words within a document.the nodes of graphs-of-words are ranked based on a modified version of the PageRank algorithm taking edge weights into account, and the top p% vertices are kept as keywords.It does not guarantee that all the main topics will be represented by the extracted keyphrases. SingleRankWan and Xiao 2008 Extended TextRank by adding weighted edges between words that co-occur in a window of variable size w ≥ 2. ExpandRankWan and Xiao 2008 Textually-similar neighboring documents are included in ExpandRank (Wan and Xiao, 2008) to compute more accurate word co-occurrence information Researchers have computed relatedness between candidates using co-occurrence counts (Mihalcea and Tarau, 2004; Matsuo and Ishizuka, 2004) and semantic relatedness (Grineva et al., 2009), and represented the relatedness information collected from a document as a graph (Mihalcea and Tarau, 2004; Wan and Xiao, 2008a; Wan and Xiao, 2008b; Bougouin et al., 2013). 2. Topic-Based Clustering KeyCluster: clusters semantically similar candidates using Wikipedia and co-occurrence-based statis- tics. Clustering to find exemplar terms for keyphrase extraction. Liu et al. (2009b) first grouping candidate words into topics and then, extracting one representative keyphrase from each topic.drawback: by extracting keyphrases from each topic cluster, it essentially gives each topic equal importance. Topical PageRank (TPR): Automatic keyphrase extraction via topic decomposition. Liu 2010 overcome drawbacks of KeyCluster. In particular, they decomposed a document into multiple topics, using topic models, and applied a separate topic-biased PageRank for each topic. The PageRank scores from each topic were then combined into a single score, using as weights the topic proportions returned by topic models for the document.TPR performs significantly better than both tf*idf and TextRank on the DUC-2001 and Inspec datasets. CommunityCluster: Extracting Key Terms From Noisy and Multi-theme Documents,Grineva et al. (2009) CommunityCluster gives more weight to more important topics, but unlike TPR, it extracts all candidate keyphrases from an important topic, assuming that a candidate that receives little focus in the text should still be extracted as a keyphrase as long as it is related to an important topic. 3. Simultaneous Learning4. Language Modelingdeserves further investigationTomokiyo and Hurst 2003Language ModelThe foreground corpus is composed of the set of documents from which keyphrases are to be extracted.The background corpus is a large corpus that encodes general knowledge about the world (e.g., the Web). LMA uses a language model rather than heuristics to identify phrases, and relies on the language model trained on the background corpus to determine how “unique” a candidate keyphrase is to the domain represented by the foreground corpus. EvaluationDataset: Inspec (Hulth, 2003): This dataset provides 2,000 paper abstracts. We adopt the 500 testing papers and their corresponding uncontrolled keyphrases for evaluation, and the remaining 1,500 papers are used for training the supervised baseline models. The Hulth 2003 Hulth, 2003 dataset contains abstracts drawn from the Inspec database of physics and engineering papers. Following our baselines, we used the 500 documents in the validation set and the “uncontrolled” keywords assigned by human annotators. The mean document size is 120 words and on average, 21 keywords (in terms of unigrams) are available for each document. Marujo 2012, containing 450 web news stories of about 440 words on average, covering 10 different topics from art and culture to business, sport, and technology (Marujo et al., 2012). For each story, the keyphrases assigned by at least 9 out of 10 Amazon Mechanical Turkers are provided as gold standard. After splitting the keyphrases into unigrams, this makes for an average of 68 keywords per document, which is much higher than for the two other datasets, even the one comprising long documents (Semeval, see next). Krapivin (Krapivin et al., 2008): This dataset provides 2,304 papers with full-text and author-assigned keyphrases. However, the author did not mention how to split testing data, so we selected the first 400 papers in alphabetical order as the testing data, and the remaining papers are used to train the su- pervised baselines. NUS (Nguyen and Kan, 2007): We use the author-assigned keyphrases and treat all 211 papers as the testing data. Since the NUS dataset did not specifically mention the ways of splitting training and testing data, the results of the supervised baseline models are obtained through a five-fold cross-validation. SemEval-2010 (Kim et al., 2010)Semeval: parsed scientific papers collected from the ACM Digital Library. Each document is approximately 1,860 words in length and is associated with about 24 keywords. KP20k: We built a new testing dataset that contains the titles, abstracts, and keyphrases of 20,000 scientific articles in computer science. They were randomly selected from our obtained 567,830 articles. Due to the memory limits of implementation, we were not able to train the supervised baselines on the whole training set. Thus we take the 20,000 articles in the validation set to train the supervised baselines. It is worth noting that we also examined their performance by enlarging the training dataset to 50,000 articles, but no significant improvement was observed. Metrics:Typical approach:(1) to create a mapping between the keyphrases in the gold standard and those in the system output using exact match, and then (2) score the output using evaluation metrics such as precision (P), recall (R), and F-score Two types of automatic evaluation metrics: With exact match. These metrics reward a partial match between a predicted keyphrase and a gold keyphrase (i.e., overlapping n-grams) and are commonly used in machine translation (MT) and summarization evaluations. They include BLEU, METEOR, NIST, and ROUGE. Nevertheless, experiments show that these MT metrics only offer a partial solution to problem with exact match: they can only detect a subset of the near-misses (Kim et al., 2010a). How a system ranks its predictions: Given that two systems A and B have the same number of correct predictions, binary preference measure (Bpref) and mean reciprocal rank (MRR) Liu et al., 2010 will award more credit to A than to B if the ranks of the correct predictions in A’s output are higher than those in B’s output. R-precision (Rp): IR metric that focuses on ranking: given a document with n gold keyphrases, it computes the precision of a system over its n highest-ranked candidates Zesch and Gurevych, 2009. The motivation behind the design of Rp is simple: a system will achieve a perfect Rp value if it ranks all the keyphrases above the non-keyphrases. The state of Art","categories":[{"name":"NLP","slug":"NLP","permalink":"http://junying.ink/categories/NLP/"}],"tags":[{"name":"Keywords Extraction","slug":"Keywords-Extraction","permalink":"http://junying.ink/tags/Keywords-Extraction/"}]},{"title":"Linux Terminal Notes","slug":"Linux-Terminal-Notes","date":"2018-03-16T13:48:50.000Z","updated":"2018-04-02T03:16:05.000Z","comments":true,"path":"2018/03/16/Linux-Terminal-Notes/","link":"","permalink":"http://junying.ink/2018/03/16/Linux-Terminal-Notes/","excerpt":"","text":"open your operating system’s file explorer in this directory.12$ open . # for MAC$ gnome-open . # for Ubuntu nohup command &gt; myout.file 2&gt;&amp;1 &amp;输出被重定向到myout.file文件中。使用 jobs 查看任务","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://junying.ink/tags/Linux/"}]},{"title":"Intro to Java","slug":"Intro-to-Java","date":"2018-03-16T13:16:14.000Z","updated":"2018-03-16T14:49:49.000Z","comments":true,"path":"2018/03/16/Intro-to-Java/","link":"","permalink":"http://junying.ink/2018/03/16/Intro-to-Java/","excerpt":"Intro to Java.CS 16B sp2018","text":"Intro to Java.CS 16B sp2018 Java program12345public class HelloWorld &#123; public static void main(String[] args) &#123; System.out.println(\"Hello world!\"); &#125;&#125; Execute Java12$ javac HelloWorld.java # Compiler would generate HelloWorld.class$ java HelloWorld # Interpreter would run the class file. Static TypingJava variables can contain values of that type, and only that type. Furthermore, the type of a variable can never change. The compiler rejects this program out of hand before it even runs.This is in contrast to dynamically typed languages like Python, where users can run into type errors during execution! FunctionsFunctions that are part of a class are commonly called “methods”. Comments, JavadocComments where appropriate. Line comments in Java use the // delimiter. Block (a.k.a. multi-line comments) comments use /* and */ JavadocThe widely used javadoc tool can be used to generate HTML descriptions of your code. 12345678910111213public class LargerDemo &#123; /** Returns the larger of x and y. */ public static int larger(int x, int y) &#123; if (x &gt; y) &#123; return x; &#125; return y; &#125; public static void main(String[] args) &#123; System.out.println(larger(8, 10)); &#125;&#125; Array12345678910int[] numbers = new int[3];numbers[0] = 4;numbers[1] = 7;numbers[2] = 10;System.out.println(numbers[1]);int[] numbers = new int[]&#123;4, 7, 10&#125;;System.out.println(numbers[1]);String[] a = &#123;\"cat\", \"dog\", \"laser horse\", \"ketchup\", \"horse\", \"horbse\"&#125;; Command Line Arguments12345public class ArgsDemo &#123; public static void main(String[] args) &#123; System.out.println(args[0]); &#125;&#125; 12$ java ArgsDemo these are command line argumentsthese","categories":[{"name":"CS 61B","slug":"CS-61B","permalink":"http://junying.ink/categories/CS-61B/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://junying.ink/tags/Java/"}]},{"title":"Markdown Note","slug":"markdown-note","date":"2018-03-16T06:03:01.000Z","updated":"2018-03-19T04:44:43.000Z","comments":true,"path":"2018/03/16/markdown-note/","link":"","permalink":"http://junying.ink/2018/03/16/markdown-note/","excerpt":"Basic markdown grammars for a quick reference.","text":"Basic markdown grammars for a quick reference. 标题 First Level Title======Second Level Title------- # Fisrt Level## Second Level#### forth level###### sixth level 段落 段落前后有空行就可以。段内强行换行：使用两个空格以上加上回车 区块引用 &gt;区块引用 &gt;&gt;可以嵌套 代码区块 每行或只有第一行加上4个空格或者一个制表符 or 代码上下两行添加两个反引号 def index(): pass 1public static void main(String[] args)&#123;&#125; 强调 两侧加上*是斜体hi两侧加上** 是粗体hi 列表 使用+加上一个空格或者制表符 红豆 大红豆 芋头 有序列表是 aa bb cc 分割线 三个或以上*、-、_ 连接 行内式: 中括号之间是代替的中文 之后紧跟一个小括号内部写链接本文的参考来源 图片 与链接相同 只是在链接前方加个！![图片信息](外部链接) 反斜杠\\ 转义，使符号成为普通符号 bash ``` bash 被围住的内容 ```123$ hexo clean$ hexo g$ hexo d 缩进 &amp;ensp;输入一个空格&amp;emsp;输入两个空格 标记 `ctrl+a`ctrl+a 数学公式在markdown写作首部添加如下代码： 之后写公式:$$公式$$表示行间公式","categories":[{"name":"Markdown","slug":"Markdown","permalink":"http://junying.ink/categories/Markdown/"}],"tags":[{"name":"Markdown","slug":"Markdown","permalink":"http://junying.ink/tags/Markdown/"}]},{"title":"Hexo Note","slug":"learn-to-write-with-hexo","date":"2018-03-16T05:32:48.000Z","updated":"2018-03-16T08:51:51.000Z","comments":true,"path":"2018/03/16/learn-to-write-with-hexo/","link":"","permalink":"http://junying.ink/2018/03/16/learn-to-write-with-hexo/","excerpt":"create new post create new page","text":"create new post create new page Write a new post with hexo.12345$ hexo new \"new article\" #open source/_posts/new-article.md and edit the post.$ hexo clean #will clean static-file in public/ and cached-file like db.json$ hexo generate #generate static files for web pages, in the public/$ hexo server #start the local server and test the page via localhost:4000$ hexo deploy #deploy Every time after you write a post, input ‘$ hexo s’ to start the server and have a check via localhost:4000 After start the server, everytime you modify the post, only refreshing the brower will work. If you are satisfied with the modification, use ‘$ hexo g’ to generate the static files. (before it, ‘$ hexo clean’ is recommended) create ‘ABOUT’ page$ hexo new page “about”在主题的_config.yml设置中，写出menu中的about menu: home: / &ensp; archives: /arvhives &ensp; tags: /tags &ensp; about: /about","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://junying.ink/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://junying.ink/tags/Hexo/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-03-15T14:25:11.000Z","updated":"2018-03-16T09:07:14.000Z","comments":true,"path":"2018/03/15/hello-world/","link":"","permalink":"http://junying.ink/2018/03/15/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://junying.ink/categories/Hexo/"}],"tags":[]}]}